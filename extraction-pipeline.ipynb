{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text and tables extraction pipeline\n",
    "\n",
    "This notebook briefly presents how to run our extraction pipeline (in both single and batch mode) to extract and work with papers' text and tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As specified in README, we have to create conda environment:\n",
    "\n",
    "```conda env create -f environment.yml```\n",
    "\n",
    "Additionally, we can pull in advance the required docker images:\n",
    "\n",
    "```make pull_images```\n",
    "\n",
    "Once the environment is activated (`conda activate xtables`) we can test the extraction pipeline with `make test`. To extract data in batch mode, run\n",
    "\n",
    "```make DATA_DIR=mydata -j 8 -i extract_all > stdout.log 2> stderr.log```\n",
    "\n",
    "where `8` is a number of processes to use. Under `mydata` there should be a directory named `arxiv/sources` containing  packed LaTeX sources, one archive per paper. After running the test above you can see the directory structure by inspecting `test/data/arxiv` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the extraction is finished we can load all the papers. In the example below we load the paper from `test` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sota_extractor2.data.paper_collection import PaperCollection\n",
    "pc = PaperCollection.from_files('test/data/arxiv', load_annotations=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PaperCollection` is a list of papers. In the test example there's only one paper with two tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DILBERT: Distilling Inner Latent BERT variables\n",
      "In this paper we achieve state-of-the-art performance in random number generation.\n"
     ]
    }
   ],
   "source": [
    "print(pc[0].text.title)\n",
    "print(pc[0].text.abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "body{margin:0;padding:0;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue,sans-serif;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}code{font-family:source-code-pro,Menlo,Monaco,Consolas,Courier New,monospace}.tableWrapper{-overflow:auto}.tableWrapper .has-annotations{color:#ff3860}.tableWrapper .model-params{background-color:#209cee;color:rgba(0,0,0,.7)}.tableWrapper .table-meta{background-color:#fff3c5;color:rgba(0,0,0,.7)}.tableWrapper .model-best{background-color:#ff3860;color:rgba(0,0,0,.7)}.tableWrapper .model-competing{background-color:#ffdd57;color:rgba(0,0,0,.7)}.tableWrapper .model-paper{background-color:#ff3860;color:#fff}.tableWrapper .dataset-sub{background-color:#23d160;color:#fff}.tableWrapper .dataset-metric{background-color:#209cee;color:#fff}.tableWrapper .dataset{background-color:#02bd43;color:#fff}.tableWrapper .trash{background-color:#363636;color:#f5f5f5}.tableWrapper .wtf{background-color:#f0f;color:#f5f5f5}.tableWrapper .dataset-task{background-color:#77ecdd;color:rgba(0,0,0,.7)}.tableWrapper .dataset-paper{background-color:#e4ffee;color:rgba(0,0,0,.7)}.tableWrapper td.focused-cell{outline:2px solid #9ecaed;border-radius:7px;box-shadow:0 0 10px #9ecaed}.tableWrapper span.text-bold{font-weight:700}.tableWrapper span.text-italic{font-style:italic}.tableWrapper span.text-red{color:red}.tableWrapper span.text-green{color:green}.tableWrapper span.text-blue{color:#00f}.predict-dataset,.predict-dataset-metric,.predict-model-competing,.predict-model-paper,.predict-model-params,.predict-table-meta{outline:2px solid #9ecaed;border-radius:7px;box-shadow:0 0 10px #9ecaed}.tableWrapper .predict-model-params{background-color:#209cee;color:rgba(0,0,0,.7)}.tableWrapper .predict-table-meta{background-color:#fff3c5;color:rgba(0,0,0,.7)}.tableWrapper .predict-model-competing{background-color:#ffdd57;color:rgba(0,0,0,.7)}.tableWrapper .predict-model-paper{background-color:#ff3860;color:#fff}.tableWrapper .predict-dataset-metric{background-color:#209cee;color:#fff}.tableWrapper .predict-dataset{background-color:#02bd43;color:#fff}.tableWrapper td{border:inherit}.tableWrapper table tr td.border-l{border-left:1px solid #000}.tableWrapper table tr td.border-r{border-right:1px solid #000}.tableWrapper table tr td.border-t{border-top:1px solid #000}.tableWrapper table tr td.border-b{border-bottom:1px solid #000}.tableWrapper table tr td.border-ll{border-left:2px solid #000}.tableWrapper table tr td.border-rr{border-right:2px solid #000}.tableWrapper table tr td.border-tt{border-top:2px solid #000}.tableWrapper table tr td.border-bb{border-bottom:2px solid #000}.tableWrapper table tr td.align-left{text-align:left}.tableWrapper table tr td.align-right{text-align:right}.tableWrapper table tr td.align-center{text-align:center}.tableWrapper table tr td.align-justify{text-align:justify}div.form-group>input.form-control.input-sm{border-radius:2px;font-size:.75rem;background-color:#fff;color:#363636;box-shadow:inset 0 1px 2px rgba(10,10,10,.1);max-width:100%;width:100%;height:2.25em;padding:calc(.375em - 1px) calc(.625em - 1px);position:relative;border:1px solid #b5b5b5}div.form-group>input.form-control.input-sm:focus{border-color:#3273dc;box-shadow:0 0 0 .125em rgba(50,115,220,.25)}.ht_clone_top{z-index:20}.evaluation-tables{overflow:scroll;max-height:20vh;border-top:1px solid #a9a9a9}.navbar.is-fixed-bottom,.navbar.is-fixed-top{z-index:200}body{padding-bottom:20vh}\n",
       ".tableWrapper .final-proposal{ background: lightgreen }\n",
       "</style>\n",
       "\n",
       "<div class=\"tableWrapper\">\n",
       "<table>\n",
       "<tr>\n",
       "<td class=\" border-tt align-left \" title=\"\">left</td>\n",
       "<td class=\" border-tt align-center \" title=\"\">center</td>\n",
       "<td class=\" border-tt align-right \" title=\"\">right</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-t align-left \" title=\"\">1</td>\n",
       "<td class=\" border-t align-center \" title=\"\">2</td>\n",
       "<td class=\" border-t align-right \" title=\"\">3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" align-left \" title=\"\">4</td>\n",
       "<td class=\" align-center \" title=\"\">5</td>\n",
       "<td class=\" align-right \" title=\"\">6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-t align-left \" title=\"\">7</td>\n",
       "<td class=\" border-t align-center \" title=\"\">8</td>\n",
       "<td class=\" border-t align-right \" title=\"\">9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-bb align-left \" title=\"\">a</td>\n",
       "<td class=\" border-bb align-center \" title=\"\">b</td>\n",
       "<td class=\" border-bb align-right \" title=\"\">c</td>\n",
       "</tr>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<sota_extractor2.data.table.Table at 0x7f60c7e7e518>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc[0].tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "body{margin:0;padding:0;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue,sans-serif;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}code{font-family:source-code-pro,Menlo,Monaco,Consolas,Courier New,monospace}.tableWrapper{-overflow:auto}.tableWrapper .has-annotations{color:#ff3860}.tableWrapper .model-params{background-color:#209cee;color:rgba(0,0,0,.7)}.tableWrapper .table-meta{background-color:#fff3c5;color:rgba(0,0,0,.7)}.tableWrapper .model-best{background-color:#ff3860;color:rgba(0,0,0,.7)}.tableWrapper .model-competing{background-color:#ffdd57;color:rgba(0,0,0,.7)}.tableWrapper .model-paper{background-color:#ff3860;color:#fff}.tableWrapper .dataset-sub{background-color:#23d160;color:#fff}.tableWrapper .dataset-metric{background-color:#209cee;color:#fff}.tableWrapper .dataset{background-color:#02bd43;color:#fff}.tableWrapper .trash{background-color:#363636;color:#f5f5f5}.tableWrapper .wtf{background-color:#f0f;color:#f5f5f5}.tableWrapper .dataset-task{background-color:#77ecdd;color:rgba(0,0,0,.7)}.tableWrapper .dataset-paper{background-color:#e4ffee;color:rgba(0,0,0,.7)}.tableWrapper td.focused-cell{outline:2px solid #9ecaed;border-radius:7px;box-shadow:0 0 10px #9ecaed}.tableWrapper span.text-bold{font-weight:700}.tableWrapper span.text-italic{font-style:italic}.tableWrapper span.text-red{color:red}.tableWrapper span.text-green{color:green}.tableWrapper span.text-blue{color:#00f}.predict-dataset,.predict-dataset-metric,.predict-model-competing,.predict-model-paper,.predict-model-params,.predict-table-meta{outline:2px solid #9ecaed;border-radius:7px;box-shadow:0 0 10px #9ecaed}.tableWrapper .predict-model-params{background-color:#209cee;color:rgba(0,0,0,.7)}.tableWrapper .predict-table-meta{background-color:#fff3c5;color:rgba(0,0,0,.7)}.tableWrapper .predict-model-competing{background-color:#ffdd57;color:rgba(0,0,0,.7)}.tableWrapper .predict-model-paper{background-color:#ff3860;color:#fff}.tableWrapper .predict-dataset-metric{background-color:#209cee;color:#fff}.tableWrapper .predict-dataset{background-color:#02bd43;color:#fff}.tableWrapper td{border:inherit}.tableWrapper table tr td.border-l{border-left:1px solid #000}.tableWrapper table tr td.border-r{border-right:1px solid #000}.tableWrapper table tr td.border-t{border-top:1px solid #000}.tableWrapper table tr td.border-b{border-bottom:1px solid #000}.tableWrapper table tr td.border-ll{border-left:2px solid #000}.tableWrapper table tr td.border-rr{border-right:2px solid #000}.tableWrapper table tr td.border-tt{border-top:2px solid #000}.tableWrapper table tr td.border-bb{border-bottom:2px solid #000}.tableWrapper table tr td.align-left{text-align:left}.tableWrapper table tr td.align-right{text-align:right}.tableWrapper table tr td.align-center{text-align:center}.tableWrapper table tr td.align-justify{text-align:justify}div.form-group>input.form-control.input-sm{border-radius:2px;font-size:.75rem;background-color:#fff;color:#363636;box-shadow:inset 0 1px 2px rgba(10,10,10,.1);max-width:100%;width:100%;height:2.25em;padding:calc(.375em - 1px) calc(.625em - 1px);position:relative;border:1px solid #b5b5b5}div.form-group>input.form-control.input-sm:focus{border-color:#3273dc;box-shadow:0 0 0 .125em rgba(50,115,220,.25)}.ht_clone_top{z-index:20}.evaluation-tables{overflow:scroll;max-height:20vh;border-top:1px solid #a9a9a9}.navbar.is-fixed-bottom,.navbar.is-fixed-top{z-index:200}body{padding-bottom:20vh}\n",
       ".tableWrapper .final-proposal{ background: lightgreen }\n",
       "</style>\n",
       "\n",
       "<div class=\"tableWrapper\">\n",
       "<table>\n",
       "<tr>\n",
       "<td class=\" border-tt align-left \" title=\"\"><span class=\"text-bold\">bold text</span></td>\n",
       "<td class=\" border-tt align-center \" title=\"\"><span class=\"text-italic\">italic text</span></td>\n",
       "<td class=\" border-tt align-right \" title=\"\"><span class=\"text-italic\"><span class=\"text-bold\">bold italic text</span></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-t align-left \" title=\"\"><span class=\"text-red\">red text</span></td>\n",
       "<td class=\" border-t align-center \" title=\"\"><span class=\"text-green\">green text</span></td>\n",
       "<td class=\" border-t align-right \" title=\"\"><span class=\"text-blue\">blue text</span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" align-left \" title=\"\"><span class=\"text-bold\">5.4%</span></td>\n",
       "<td class=\" align-center \" title=\"\"><span class=\"text-italic\">3.8%</span></td>\n",
       "<td class=\" align-right \" title=\"\"><span class=\"text-bold\">11.2</span>±0.15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-bb border-t align-left \" title=\"\"><span class=\"text-bold\">an <span class=\"text-italic\">italic</span> text inside bold</span></td>\n",
       "<td class=\" border-bb border-t align-center \" title=\"\"><span class=\"text-red\"><span class=\"text-bold\">bold red</span></span></td>\n",
       "<td class=\" border-bb border-t \" title=\"\"></td>\n",
       "</tr>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<sota_extractor2.data.table.Table at 0x7f60c7e89160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc[0].tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual extraction\n",
    "\n",
    "We can access the extraction pipeline programatically. In case of batch mode presented above we don't provide downloading of arXiv papers on purpose as for batch mode one should use arXiv's S3 bucket. In the example below we show how to load a paper directly from arXiv url, but use of it should be limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sota_extractor2.data.paper_collection import PaperCollection, TempPaper\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from sota_extractor2.helpers import LatexConverter, Unpack\n",
    "from sota_extractor2.errors import *\n",
    "from urllib.request import urlretrieve as download_url\n",
    "\n",
    "unpack = Unpack()\n",
    "latex = LatexConverter(Path().absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "arxiv_url_re = re.compile(r\"^(https?://(www.)?arxiv.org/(abs|pdf|e-print)/)?(?P<arxiv_id>\\d{4}\\.\\d+(v\\d+)?)(\\.pdf)?$\")\n",
    "\n",
    "def get_paper_html(path):\n",
    "    \"\"\"Get an HTML source of a given paper identified by either local path or arXiv url.\n",
    "    \n",
    "    :param path - either a local path to an archive with LaTeX source or arXiv url / id.\n",
    "    In the later case the LaTeX source is downloaded automatically.\n",
    "    \n",
    "    :returns html - a string containing paper's HTML source code.\n",
    "    \"\"\"\n",
    "    with TemporaryDirectory() as workdir:\n",
    "        workdir = Path(workdir)\n",
    "        m = arxiv_url_re.match(path)\n",
    "        if m:\n",
    "            path = workdir / \"source\"\n",
    "            workdir = workdir / \"unpack\"\n",
    "            download_url(f\"https://arxiv.org/e-print/{m['arxiv_id']}\", path)                \n",
    "        unpack(path, workdir)\n",
    "        html = latex.to_html(workdir)\n",
    "        # Skipped step - reference normalization\n",
    "    return html\n",
    "\n",
    "def pipeline(path):\n",
    "    \"\"\"Run the whole pipeline, from path to proposals\n",
    "\n",
    "    :param path - either a local path to an archive with LaTeX source or arXiv url / id.\n",
    "    \n",
    "    :returns paper - Paper object with labelled tables\n",
    "    :returns proposals - DataFrame containing extracted sota records\n",
    "    \"\"\"\n",
    "    try:\n",
    "        html = get_paper_html(path)\n",
    "        paper = TempPaper(html)\n",
    "        return paper\n",
    "    except UnpackError:\n",
    "        print(\"Unable to unpack sources\")\n",
    "        return None\n",
    "    except LatexConversionError:\n",
    "        print(\"Unable to convert to html\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extraction may take up to a few minutes, as it needs to:\n",
    "* download the LaTeX source\n",
    "* unpack them\n",
    "* convert LaTeX to html\n",
    "* clean the html\n",
    "* extract paper's text, title, abstract, sections and references\n",
    "* extract tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/xxt/lib/python3.7/site-packages/bs4/__init__.py:177: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    }
   ],
   "source": [
    "paper = pipeline(\"https://arxiv.org/abs/1903.08469\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Defense of Pre-trained ImageNet Architecturesfor Real-time Semantic Segmentation of Road-driving Images'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.text.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "body{margin:0;padding:0;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue,sans-serif;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}code{font-family:source-code-pro,Menlo,Monaco,Consolas,Courier New,monospace}.tableWrapper{-overflow:auto}.tableWrapper .has-annotations{color:#ff3860}.tableWrapper .model-params{background-color:#209cee;color:rgba(0,0,0,.7)}.tableWrapper .table-meta{background-color:#fff3c5;color:rgba(0,0,0,.7)}.tableWrapper .model-best{background-color:#ff3860;color:rgba(0,0,0,.7)}.tableWrapper .model-competing{background-color:#ffdd57;color:rgba(0,0,0,.7)}.tableWrapper .model-paper{background-color:#ff3860;color:#fff}.tableWrapper .dataset-sub{background-color:#23d160;color:#fff}.tableWrapper .dataset-metric{background-color:#209cee;color:#fff}.tableWrapper .dataset{background-color:#02bd43;color:#fff}.tableWrapper .trash{background-color:#363636;color:#f5f5f5}.tableWrapper .wtf{background-color:#f0f;color:#f5f5f5}.tableWrapper .dataset-task{background-color:#77ecdd;color:rgba(0,0,0,.7)}.tableWrapper .dataset-paper{background-color:#e4ffee;color:rgba(0,0,0,.7)}.tableWrapper td.focused-cell{outline:2px solid #9ecaed;border-radius:7px;box-shadow:0 0 10px #9ecaed}.tableWrapper span.text-bold{font-weight:700}.tableWrapper span.text-italic{font-style:italic}.tableWrapper span.text-red{color:red}.tableWrapper span.text-green{color:green}.tableWrapper span.text-blue{color:#00f}.predict-dataset,.predict-dataset-metric,.predict-model-competing,.predict-model-paper,.predict-model-params,.predict-table-meta{outline:2px solid #9ecaed;border-radius:7px;box-shadow:0 0 10px #9ecaed}.tableWrapper .predict-model-params{background-color:#209cee;color:rgba(0,0,0,.7)}.tableWrapper .predict-table-meta{background-color:#fff3c5;color:rgba(0,0,0,.7)}.tableWrapper .predict-model-competing{background-color:#ffdd57;color:rgba(0,0,0,.7)}.tableWrapper .predict-model-paper{background-color:#ff3860;color:#fff}.tableWrapper .predict-dataset-metric{background-color:#209cee;color:#fff}.tableWrapper .predict-dataset{background-color:#02bd43;color:#fff}.tableWrapper td{border:inherit}.tableWrapper table tr td.border-l{border-left:1px solid #000}.tableWrapper table tr td.border-r{border-right:1px solid #000}.tableWrapper table tr td.border-t{border-top:1px solid #000}.tableWrapper table tr td.border-b{border-bottom:1px solid #000}.tableWrapper table tr td.border-ll{border-left:2px solid #000}.tableWrapper table tr td.border-rr{border-right:2px solid #000}.tableWrapper table tr td.border-tt{border-top:2px solid #000}.tableWrapper table tr td.border-bb{border-bottom:2px solid #000}.tableWrapper table tr td.align-left{text-align:left}.tableWrapper table tr td.align-right{text-align:right}.tableWrapper table tr td.align-center{text-align:center}.tableWrapper table tr td.align-justify{text-align:justify}div.form-group>input.form-control.input-sm{border-radius:2px;font-size:.75rem;background-color:#fff;color:#363636;box-shadow:inset 0 1px 2px rgba(10,10,10,.1);max-width:100%;width:100%;height:2.25em;padding:calc(.375em - 1px) calc(.625em - 1px);position:relative;border:1px solid #b5b5b5}div.form-group>input.form-control.input-sm:focus{border-color:#3273dc;box-shadow:0 0 0 .125em rgba(50,115,220,.25)}.ht_clone_top{z-index:20}.evaluation-tables{overflow:scroll;max-height:20vh;border-top:1px solid #a9a9a9}.navbar.is-fixed-bottom,.navbar.is-fixed-top{z-index:200}body{padding-bottom:20vh}\n",
       ".tableWrapper .final-proposal{ background: lightgreen }\n",
       "</style>\n",
       "\n",
       "<div class=\"tableWrapper\">\n",
       "<table>\n",
       "<tr>\n",
       "<td class=\" border-r align-left header \" title=\"\">model</td>\n",
       "<td class=\" align-center header \" title=\"\">subset</td>\n",
       "<td class=\" align-center header \" title=\"\">mIoU</td>\n",
       "<td class=\" align-center header \" title=\"\">FPS</td>\n",
       "<td class=\" align-center header \" title=\"\">FPS norm</td>\n",
       "<td class=\" align-center header \" title=\"\">GPU</td>\n",
       "<td class=\" align-center header \" title=\"\">resolution</td>\n",
       "<td class=\" align-center header \" title=\"\">GFLOPs</td>\n",
       "<td class=\" align-center header \" title=\"\">GFLOPs@1Mpx</td>\n",
       "<td class=\" align-center header \" title=\"\"># params</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r border-tt align-left header \" title=\"\">D* [<a title=\"bib-bib35\">35</a>]</td>\n",
       "<td class=\" border-tt align-center \" title=\"\">val</td>\n",
       "<td class=\" border-tt align-center \" title=\"\">68.4</td>\n",
       "<td class=\" border-tt align-center \" title=\"\">-</td>\n",
       "<td class=\" border-tt align-center \" title=\"\">-</td>\n",
       "<td class=\" border-tt align-center \" title=\"\">TitanX M</td>\n",
       "<td class=\" border-tt align-center \" title=\"\">1024x512</td>\n",
       "<td class=\" border-tt align-center \" title=\"\">5.8</td>\n",
       "<td class=\" border-tt align-center \" title=\"\">11.6</td>\n",
       "<td class=\" border-tt align-center \" title=\"\">0.5M</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r align-left header \" title=\"\">DG2s [<a title=\"bib-bib35\">35</a>]</td>\n",
       "<td class=\" align-center \" title=\"\">val</td>\n",
       "<td class=\" align-center \" title=\"\">70.6</td>\n",
       "<td class=\" align-center \" title=\"\">-</td>\n",
       "<td class=\" align-center \" title=\"\">-</td>\n",
       "<td class=\" align-center \" title=\"\">TitanX M</td>\n",
       "<td class=\" align-center \" title=\"\">1024x512</td>\n",
       "<td class=\" align-center \" title=\"\">19.0</td>\n",
       "<td class=\" align-center \" title=\"\">38</td>\n",
       "<td class=\" align-center \" title=\"\">1.2M</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r align-left header \" title=\"\">Ladder DenseNet†[<a title=\"bib-bib16\">16</a>]</td>\n",
       "<td class=\" align-center \" title=\"\">val</td>\n",
       "<td class=\" align-center \" title=\"\">72.8</td>\n",
       "<td class=\" align-center \" title=\"\">31.0</td>\n",
       "<td class=\" align-center \" title=\"\">30.1</td>\n",
       "<td class=\" align-center \" title=\"\">TitanX</td>\n",
       "<td class=\" align-center \" title=\"\">1024x512</td>\n",
       "<td class=\" align-center \" title=\"\">-</td>\n",
       "<td class=\" align-center \" title=\"\">-</td>\n",
       "<td class=\" align-center \" title=\"\">9.8M</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r align-left header \" title=\"\">ICNet [<a title=\"bib-bib40\">40</a>]</td>\n",
       "<td class=\" align-center \" title=\"\">test</td>\n",
       "<td class=\" align-center \" title=\"\">69.5</td>\n",
       "<td class=\" align-center \" title=\"\">30.3</td>\n",
       "<td class=\" align-center \" title=\"\">49.7</td>\n",
       "<td class=\" align-center \" title=\"\">TitanX M</td>\n",
       "<td class=\" align-center \" title=\"\">2048x1024</td>\n",
       "<td class=\" align-center \" title=\"\">-</td>\n",
       "<td class=\" align-center \" title=\"\">-</td>\n",
       "<td class=\" align-center \" title=\"\">-</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r align-left header \" title=\"\">ESPNet [<a title=\"bib-bib24\">24</a>]</td>\n",
       "<td class=\" align-center \" title=\"\">test</td>\n",
       "<td class=\" align-center \" title=\"\">60.3</td>\n",
       "<td class=\" align-center \" title=\"\">112</td>\n",
       "<td class=\" align-center \" title=\"\">108.7</td>\n",
       "<td class=\" align-center \" title=\"\">TitanX</td>\n",
       "<td class=\" align-center \" title=\"\">1024x512</td>\n",
       "<td class=\" align-center \" title=\"\">-</td>\n",
       "<td class=\" align-center \" title=\"\">-</td>\n",
       "<td class=\" align-center \" title=\"\">0.4M</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r align-left header \" title=\"\">ERFNet [<a title=\"bib-bib28\">28</a>]</td>\n",
       "<td class=\" align-center \" title=\"\">test</td>\n",
       "<td class=\" align-center \" title=\"\">68.0</td>\n",
       "<td class=\" align-center \" title=\"\">11.2</td>\n",
       "<td class=\" align-center \" title=\"\">18.4</td>\n",
       "<td class=\" align-center \" title=\"\">TitanX M</td>\n",
       "<td class=\" align-center \" title=\"\">1024x512</td>\n",
       "<td class=\" align-center \" title=\"\">27.7</td>\n",
       "<td class=\" align-center \" title=\"\">55.4</td>\n",
       "<td class=\" align-center \" title=\"\">20M</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r border-t align-left header \" title=\"\">GUNet†[<a title=\"bib-bib23\">23</a>]</td>\n",
       "<td class=\" border-t align-center \" title=\"\">test</td>\n",
       "<td class=\" border-t align-center \" title=\"\">70.4</td>\n",
       "<td class=\" border-t align-center \" title=\"\">37.3</td>\n",
       "<td class=\" border-t align-center \" title=\"\">33.3</td>\n",
       "<td class=\" border-t align-center \" title=\"\">TitanXP</td>\n",
       "<td class=\" border-t align-center \" title=\"\">1024x512</td>\n",
       "<td class=\" border-t align-center \" title=\"\">-</td>\n",
       "<td class=\" border-t align-center \" title=\"\">-</td>\n",
       "<td class=\" border-t align-center \" title=\"\">-</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r align-left header \" title=\"\">ERFNet†[<a title=\"bib-bib28\">28</a>]</td>\n",
       "<td class=\" align-center \" title=\"\">test</td>\n",
       "<td class=\" align-center \" title=\"\">69.7</td>\n",
       "<td class=\" align-center \" title=\"\">11.2</td>\n",
       "<td class=\" align-center \" title=\"\">18.4</td>\n",
       "<td class=\" align-center \" title=\"\">TitanX M</td>\n",
       "<td class=\" align-center \" title=\"\">1024x512</td>\n",
       "<td class=\" align-center \" title=\"\">27.7</td>\n",
       "<td class=\" align-center \" title=\"\">55.4</td>\n",
       "<td class=\" align-center \" title=\"\">20M</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r border-t align-left header \" title=\"\">SwiftNetRN-18</td>\n",
       "<td class=\" border-t align-center \" title=\"\">val</td>\n",
       "<td class=\" border-t align-center \" title=\"\">70.4</td>\n",
       "<td class=\" border-t align-center \" title=\"\">39.9</td>\n",
       "<td class=\" border-t align-center \" title=\"\">39.3</td>\n",
       "<td class=\" border-t align-center span-rb \" title=\"\">GTX 1080Ti</td>\n",
       "<td class=\" border-t align-center \" title=\"\">2048x1024</td>\n",
       "<td class=\" border-t align-center \" title=\"\">104.0</td>\n",
       "<td class=\" border-t align-center \" title=\"\">52.0</td>\n",
       "<td class=\" border-t align-center \" title=\"\">11.8M</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r align-left header \" title=\"\">SwiftNetMN V2</td>\n",
       "<td class=\" align-center \" title=\"\">val</td>\n",
       "<td class=\" align-center \" title=\"\">69.4</td>\n",
       "<td class=\" align-center \" title=\"\">27.7</td>\n",
       "<td class=\" align-center \" title=\"\">27.7</td>\n",
       "<td class=\" align-center span-re \" title=\"\">GTX 1080Ti</td>\n",
       "<td class=\" align-center \" title=\"\">2048x1024</td>\n",
       "<td class=\" align-center \" title=\"\">41.0</td>\n",
       "<td class=\" align-center \" title=\"\">20.5</td>\n",
       "<td class=\" align-center \" title=\"\">2.4M</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r border-t align-left header \" title=\"\">SwiftNetRN-18†</td>\n",
       "<td class=\" border-t align-center \" title=\"\">val</td>\n",
       "<td class=\" border-t align-center \" title=\"\">70.2</td>\n",
       "<td class=\" border-t align-center \" title=\"\">134.9</td>\n",
       "<td class=\" border-t align-center \" title=\"\">134.9</td>\n",
       "<td class=\" border-t align-center span-rb \" title=\"\">GTX 1080Ti</td>\n",
       "<td class=\" border-t align-center \" title=\"\">1024x512</td>\n",
       "<td class=\" border-t align-center \" title=\"\">26.0</td>\n",
       "<td class=\" border-t align-center \" title=\"\">52.0</td>\n",
       "<td class=\" border-t align-center \" title=\"\">11.8M</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r align-left header \" title=\"\">SwiftNetRN-18 pyr†</td>\n",
       "<td class=\" align-center \" title=\"\">val</td>\n",
       "<td class=\" align-center \" title=\"\">74.4</td>\n",
       "<td class=\" align-center \" title=\"\">34.0</td>\n",
       "<td class=\" align-center \" title=\"\">34.0</td>\n",
       "<td class=\" align-center span-ri \" title=\"\">GTX 1080Ti</td>\n",
       "<td class=\" align-center \" title=\"\">2048x1024</td>\n",
       "<td class=\" align-center \" title=\"\">114.0</td>\n",
       "<td class=\" align-center \" title=\"\">57.0</td>\n",
       "<td class=\" align-center \" title=\"\">12.9M</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r align-left header \" title=\"\">SwiftNetMN V2†</td>\n",
       "<td class=\" align-center \" title=\"\">val</td>\n",
       "<td class=\" align-center \" title=\"\">75.3</td>\n",
       "<td class=\" align-center \" title=\"\">27.7</td>\n",
       "<td class=\" align-center \" title=\"\">27.7</td>\n",
       "<td class=\" align-center span-ri \" title=\"\">GTX 1080Ti</td>\n",
       "<td class=\" align-center \" title=\"\">2048x1024</td>\n",
       "<td class=\" align-center \" title=\"\">41.0</td>\n",
       "<td class=\" align-center \" title=\"\">20.5</td>\n",
       "<td class=\" align-center \" title=\"\">2.4M</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r align-left header \" title=\"\">SwiftNetRN-18†</td>\n",
       "<td class=\" align-center \" title=\"\">val</td>\n",
       "<td class=\" align-center \" title=\"\">75.4</td>\n",
       "<td class=\" align-center \" title=\"\">39.9</td>\n",
       "<td class=\" align-center \" title=\"\">39.3</td>\n",
       "<td class=\" align-center span-re \" title=\"\">GTX 1080Ti</td>\n",
       "<td class=\" align-center \" title=\"\">2048x1024</td>\n",
       "<td class=\" align-center \" title=\"\">104.0</td>\n",
       "<td class=\" align-center \" title=\"\">52.0</td>\n",
       "<td class=\" align-center \" title=\"\">11.8M</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r border-t align-left header \" title=\"\">SwiftNetRN-18 pyr†</td>\n",
       "<td class=\" border-t align-center \" title=\"\">test</td>\n",
       "<td class=\" border-t align-center \" title=\"\">75.1</td>\n",
       "<td class=\" border-t align-center \" title=\"\">34.0</td>\n",
       "<td class=\" border-t align-center \" title=\"\">34.0</td>\n",
       "<td class=\" border-t align-center span-rb \" title=\"\">GTX 1080Ti</td>\n",
       "<td class=\" border-t align-center \" title=\"\">2048x1024</td>\n",
       "<td class=\" border-t align-center \" title=\"\">114.0</td>\n",
       "<td class=\" border-t align-center \" title=\"\">57.0</td>\n",
       "<td class=\" border-t align-center \" title=\"\">12.9M</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r align-left header \" title=\"\">SwiftNetRN-18†</td>\n",
       "<td class=\" align-center \" title=\"\">test</td>\n",
       "<td class=\" align-center \" title=\"\"><span class=\"text-bold\">75.5</span></td>\n",
       "<td class=\" align-center \" title=\"\">39.9</td>\n",
       "<td class=\" align-center \" title=\"\">39.3</td>\n",
       "<td class=\" align-center span-ri \" title=\"\">GTX 1080Ti</td>\n",
       "<td class=\" align-center \" title=\"\">2048x1024</td>\n",
       "<td class=\" align-center \" title=\"\">104.0</td>\n",
       "<td class=\" align-center \" title=\"\">52.0</td>\n",
       "<td class=\" align-center \" title=\"\">11.8M</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\" border-r align-left header \" title=\"\">SwiftNetRN-18 ens†</td>\n",
       "<td class=\" align-center \" title=\"\">test</td>\n",
       "<td class=\" align-center \" title=\"\"><span class=\"text-bold\">76.5</span></td>\n",
       "<td class=\" align-center \" title=\"\">18.4</td>\n",
       "<td class=\" align-center \" title=\"\">18.4</td>\n",
       "<td class=\" align-center span-re \" title=\"\">GTX 1080Ti</td>\n",
       "<td class=\" align-center \" title=\"\">2048x1024</td>\n",
       "<td class=\" align-center \" title=\"\">218.0</td>\n",
       "<td class=\" align-center \" title=\"\">109.0</td>\n",
       "<td class=\" align-center \" title=\"\">24.7M</td>\n",
       "</tr>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<sota_extractor2.data.table.Table at 0x7f60c6ca4908>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally the cells are stored as a pandas dataframe of `Cell`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cell(value='model', raw_value='model', gold_tags='', refs=[], layout='border-r align-left header')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.tables[0].df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model</td>\n",
       "      <td>subset</td>\n",
       "      <td>mIoU</td>\n",
       "      <td>FPS</td>\n",
       "      <td>FPS norm</td>\n",
       "      <td>GPU</td>\n",
       "      <td>resolution</td>\n",
       "      <td>GFLOPs</td>\n",
       "      <td>GFLOPs@1Mpx</td>\n",
       "      <td># params</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D* [35]</td>\n",
       "      <td>val</td>\n",
       "      <td>68.4</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>TitanX M</td>\n",
       "      <td>1024x512</td>\n",
       "      <td>5.8</td>\n",
       "      <td>11.6</td>\n",
       "      <td>0.5M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DG2s [35]</td>\n",
       "      <td>val</td>\n",
       "      <td>70.6</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>TitanX M</td>\n",
       "      <td>1024x512</td>\n",
       "      <td>19.0</td>\n",
       "      <td>38</td>\n",
       "      <td>1.2M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ladder DenseNet†[16]</td>\n",
       "      <td>val</td>\n",
       "      <td>72.8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>TitanX</td>\n",
       "      <td>1024x512</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>9.8M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICNet [40]</td>\n",
       "      <td>test</td>\n",
       "      <td>69.5</td>\n",
       "      <td>30.3</td>\n",
       "      <td>49.7</td>\n",
       "      <td>TitanX M</td>\n",
       "      <td>2048x1024</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ESPNet [24]</td>\n",
       "      <td>test</td>\n",
       "      <td>60.3</td>\n",
       "      <td>112</td>\n",
       "      <td>108.7</td>\n",
       "      <td>TitanX</td>\n",
       "      <td>1024x512</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.4M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ERFNet [28]</td>\n",
       "      <td>test</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>18.4</td>\n",
       "      <td>TitanX M</td>\n",
       "      <td>1024x512</td>\n",
       "      <td>27.7</td>\n",
       "      <td>55.4</td>\n",
       "      <td>20M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GUNet†[23]</td>\n",
       "      <td>test</td>\n",
       "      <td>70.4</td>\n",
       "      <td>37.3</td>\n",
       "      <td>33.3</td>\n",
       "      <td>TitanXP</td>\n",
       "      <td>1024x512</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ERFNet†[28]</td>\n",
       "      <td>test</td>\n",
       "      <td>69.7</td>\n",
       "      <td>11.2</td>\n",
       "      <td>18.4</td>\n",
       "      <td>TitanX M</td>\n",
       "      <td>1024x512</td>\n",
       "      <td>27.7</td>\n",
       "      <td>55.4</td>\n",
       "      <td>20M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SwiftNetRN-18</td>\n",
       "      <td>val</td>\n",
       "      <td>70.4</td>\n",
       "      <td>39.9</td>\n",
       "      <td>39.3</td>\n",
       "      <td>GTX 1080Ti</td>\n",
       "      <td>2048x1024</td>\n",
       "      <td>104.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>11.8M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SwiftNetMN V2</td>\n",
       "      <td>val</td>\n",
       "      <td>69.4</td>\n",
       "      <td>27.7</td>\n",
       "      <td>27.7</td>\n",
       "      <td>GTX 1080Ti</td>\n",
       "      <td>2048x1024</td>\n",
       "      <td>41.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.4M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SwiftNetRN-18†</td>\n",
       "      <td>val</td>\n",
       "      <td>70.2</td>\n",
       "      <td>134.9</td>\n",
       "      <td>134.9</td>\n",
       "      <td>GTX 1080Ti</td>\n",
       "      <td>1024x512</td>\n",
       "      <td>26.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>11.8M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SwiftNetRN-18 pyr†</td>\n",
       "      <td>val</td>\n",
       "      <td>74.4</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>GTX 1080Ti</td>\n",
       "      <td>2048x1024</td>\n",
       "      <td>114.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>12.9M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SwiftNetMN V2†</td>\n",
       "      <td>val</td>\n",
       "      <td>75.3</td>\n",
       "      <td>27.7</td>\n",
       "      <td>27.7</td>\n",
       "      <td>GTX 1080Ti</td>\n",
       "      <td>2048x1024</td>\n",
       "      <td>41.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.4M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SwiftNetRN-18†</td>\n",
       "      <td>val</td>\n",
       "      <td>75.4</td>\n",
       "      <td>39.9</td>\n",
       "      <td>39.3</td>\n",
       "      <td>GTX 1080Ti</td>\n",
       "      <td>2048x1024</td>\n",
       "      <td>104.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>11.8M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SwiftNetRN-18 pyr†</td>\n",
       "      <td>test</td>\n",
       "      <td>75.1</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>GTX 1080Ti</td>\n",
       "      <td>2048x1024</td>\n",
       "      <td>114.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>12.9M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SwiftNetRN-18†</td>\n",
       "      <td>test</td>\n",
       "      <td>75.5</td>\n",
       "      <td>39.9</td>\n",
       "      <td>39.3</td>\n",
       "      <td>GTX 1080Ti</td>\n",
       "      <td>2048x1024</td>\n",
       "      <td>104.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>11.8M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SwiftNetRN-18 ens†</td>\n",
       "      <td>test</td>\n",
       "      <td>76.5</td>\n",
       "      <td>18.4</td>\n",
       "      <td>18.4</td>\n",
       "      <td>GTX 1080Ti</td>\n",
       "      <td>2048x1024</td>\n",
       "      <td>218.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>24.7M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0       1     2      3         4           5  \\\n",
       "0                  model  subset  mIoU    FPS  FPS norm         GPU   \n",
       "1                D* [35]     val  68.4      -         -    TitanX M   \n",
       "2              DG2s [35]     val  70.6      -         -    TitanX M   \n",
       "3   Ladder DenseNet†[16]     val  72.8   31.0      30.1      TitanX   \n",
       "4             ICNet [40]    test  69.5   30.3      49.7    TitanX M   \n",
       "5            ESPNet [24]    test  60.3    112     108.7      TitanX   \n",
       "6            ERFNet [28]    test  68.0   11.2      18.4    TitanX M   \n",
       "7             GUNet†[23]    test  70.4   37.3      33.3     TitanXP   \n",
       "8            ERFNet†[28]    test  69.7   11.2      18.4    TitanX M   \n",
       "9          SwiftNetRN-18     val  70.4   39.9      39.3  GTX 1080Ti   \n",
       "10         SwiftNetMN V2     val  69.4   27.7      27.7  GTX 1080Ti   \n",
       "11        SwiftNetRN-18†     val  70.2  134.9     134.9  GTX 1080Ti   \n",
       "12    SwiftNetRN-18 pyr†     val  74.4   34.0      34.0  GTX 1080Ti   \n",
       "13        SwiftNetMN V2†     val  75.3   27.7      27.7  GTX 1080Ti   \n",
       "14        SwiftNetRN-18†     val  75.4   39.9      39.3  GTX 1080Ti   \n",
       "15    SwiftNetRN-18 pyr†    test  75.1   34.0      34.0  GTX 1080Ti   \n",
       "16        SwiftNetRN-18†    test  75.5   39.9      39.3  GTX 1080Ti   \n",
       "17    SwiftNetRN-18 ens†    test  76.5   18.4      18.4  GTX 1080Ti   \n",
       "\n",
       "             6       7            8         9  \n",
       "0   resolution  GFLOPs  GFLOPs@1Mpx  # params  \n",
       "1     1024x512     5.8         11.6      0.5M  \n",
       "2     1024x512    19.0           38      1.2M  \n",
       "3     1024x512       -            -      9.8M  \n",
       "4    2048x1024       -            -         -  \n",
       "5     1024x512       -            -      0.4M  \n",
       "6     1024x512    27.7         55.4       20M  \n",
       "7     1024x512       -            -         -  \n",
       "8     1024x512    27.7         55.4       20M  \n",
       "9    2048x1024   104.0         52.0     11.8M  \n",
       "10   2048x1024    41.0         20.5      2.4M  \n",
       "11    1024x512    26.0         52.0     11.8M  \n",
       "12   2048x1024   114.0         57.0     12.9M  \n",
       "13   2048x1024    41.0         20.5      2.4M  \n",
       "14   2048x1024   104.0         52.0     11.8M  \n",
       "15   2048x1024   114.0         57.0     12.9M  \n",
       "16   2048x1024   104.0         52.0     11.8M  \n",
       "17   2048x1024   218.0        109.0     24.7M  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.tables[0].df.applymap(lambda cell: cell.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper's text can be accessed as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S1 1 Introduction"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-S0F1 Figure 1: Speed-accuracy trade-off for different semantic semantic segmentation approaches on Cityscapes test on GTX1080Ti (except for DG2s which reports only validation performance). Red dots represent our method. Other methods are displayed in green, whereas blue dots show estimated frame rates of the corresponding methods on our GPU (please refer to subsection xxref-S4SS2 for details). Our submissions achieve the best accuracy and the best speed among all approaches aiming at real-time operation. Semantic segmentation is an important dense prediction task in which the inference targets posterior distribution over a known set of classes in each image pixel [xxref-bibbib6, xxref-bibbib20, xxref-bibbib3]. Currently, the best results are achieved with deep fully convolutional models which require extraordinary computational resources. Many important applications such as autonomous navigation or driver assistance require inference on very large images in order to cover a wide field of view and perceive pedestrians at distances of over 200m. At the same time, these applications require a very low latency in order to be able to bring real-time decisions. The resulting requirements intensify computational strain and make real-time implementations a challenging research objective."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "1000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Many real-time semantic segmentation approaches [xxref-bibbib28, xxref-bibbib40, xxref-bibbib24, xxref-bibbib32] address this goal by introducing custom lightweight architectures which are not suited for large-scale visual recognition. Most of these approaches initialize training from scratch, and thus miss a huge regularization opportunity offered by knowledge transfer [xxref-bibbib26] from larger and more diverse recognition datasets [xxref-bibbib30]. Consequently, these methods incur a comparatively large overfitting risk. Some approaches alleviate this shortcoming by pre-training on ImageNet [xxref-bibbib28]. However, our experiments suggest that the resulting benefits tend to be smaller than in architectures which are designed for competitive ImageNet performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "1001"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "A simple model for semantic segmentation starts with a fully convolutional encoder which gradually decreases the resolution and increases the number of feature maps of the resulting representation. Instead of performing global pooling (as we would do in image-wide classification) one can proceed by attaching a pixel-wise loss to obtain the predictions [xxref-bibbib6]. The resulting model would lead to a very fast evaluation on modern hardware, however its accuracy would be rather low due to the following problems. Firstly, small objects (e.g. distant traffic signs) would not be recognized due to low resolution of pixel-wise predictions, which is usually 32 times smaller than the input image. Secondly, the receptive field of such models would not be large enough to classify pixels at large objects (e.g. nearby buses or trucks). These problems can be alleviated with various techniques such as dilated convolutions [xxref-bibbib37], learned upsampling [xxref-bibbib20], lateral connections [xxref-bibbib27, xxref-bibbib29, xxref-bibbib19, xxref-bibbib16] and resolution pyramids [xxref-bibbib6]. However, not all of these techniques are equally suited for real-time operation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "1002"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "In this paper, we argue that a competitive blend of efficiency and prediction accuracy can be achieved by models based on lightweight ImageNet-grade classification architectures [xxref-bibbib8, xxref-bibbib31]. Additionally, we propose a novel approach to increase the receptive field of deep model predictions, based on a resolution pyramid with shared parameters [xxref-bibbib6]. The proposed approach incurs a very modest increase of the model capacity and is therefore especially suited for datasets with large objects and few annotated images. Finally, we show that the resolution of the predictions can be efficiently and accurately upsampled by a lightweight decoder with lateral connections [xxref-bibbib27, xxref-bibbib29]. We show that the resulting semantic segmentation models can be evaluated under various computing budgets, and that they are feasible even on embedded GPU platforms. We present experiments both with ImageNet pre-training and learning from scratch on different road driving datasets. Our experiments achieve state-of-the art semantic segmentation accuracy among all existing approaches aiming at real-time execution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "1003"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S2 2 Related Work"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "As described in the introduction, semantic segmentation models have to face two major problems: restoring the input resolution and increasing the receptive field. The simplest way to restore input resolution is to avoid downsampling. This is usually achieved by replacing stride-2 poolings with non-strided poolings, and doubling the dilation factor in subsequent convolutions [xxref-bibbib4, xxref-bibbib38]. However, this approach increases the resolution of deep latent representations, which implies a large computational complexity. Furthermore, dilated convolutions incur significant additional slow-down due to necessity to rearrange the image data before and after calling optimized implementations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Another way to achieve dense image prediction relies on trained upsampling [xxref-bibbib20], which results in an encoder-decoder architecture. The idea is to perform recognition on subsampled latent representation to reduce complexity and then to restore the resolution by upsampling the representation (or the predictions). This setup can be naturally augmented by introducing lateral connections [xxref-bibbib27, xxref-bibbib29, xxref-bibbib19, xxref-bibbib16] to blend semantically rich deep layers with spatially rich shallow layers. The upsampling path has to be as lean as possible (to achieve efficiency and prevent overfitting) but no leaner (to avoid underfitting). It turns out that the sweet spot is computationally inexpensive, which makes this setup especially well-suited for real-time operation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2001"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Early approaches to enlarge the receptive field of logit activations were based on dilated convolutions [xxref-bibbib37, xxref-bibbib4, xxref-bibbib38]. A more involved approach is known as spatial pyramid pooling (SPP) [xxref-bibbib7]. SPP averages features over aligned grids with different granularities [xxref-bibbib17]. We use a convolutional adaptation of that idea, in which the feature pyramid is upsampled to the original resolution [xxref-bibbib41] and then concatenated with the input features. Thus, subsequent convolutions obtain access to broad spatial pools and that increases their receptive field. The combination of dilated convolutions and SPP is known as à trous SPP, or ASPP for short [xxref-bibbib3]. However, SPP and ASPP may hurt generalization due to large capacity. In this paper, we study resolution pyramids as an alternative way to increase the receptive field and at the same time promote scale invariance [xxref-bibbib34, xxref-bibbib15, xxref-bibbib18, xxref-bibbib4]. Most previous pyramidal approaches to semantic segmentation [xxref-bibbib6, xxref-bibbib40, xxref-bibbib23] fuse only the deepest representations extracted at different resolutions. Different from them, we combine representations from different abstraction levels before joining the upsampling path within the decoder. This results in a better gradient flow throughout the pyramidal representation which is advantageous when the objects are large and the training data is scarce."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2002"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Efficient recognition architectures leverage optimized building blocks which aim at reducing computational load while preserving accuracy. Grouped convolutions reduce the number of floating point operations and the number of parameters by enclosing the information flow within smaller groups of feature maps. Various methods have been proposed to discover prominent inter-group connections. ShuffleNet [xxref-bibbib39] uses channel shuffling to pass information across convolutional groups. CondenseNet [xxref-bibbib11] incorporates a training strategy which locates important connections in grouped convolutions and prunes those which are redundant. Neural architecture search [xxref-bibbib42] is an approach that leverages reinforcement learning to jointly learn the model architecture and the corresponding parameters. The resulting architectures achieve competitive ImageNet performance when the computational budget is restricted. Depthwise separable convolutions [xxref-bibbib33, xxref-bibbib36] decrease computational complexity by splitting a regular convolution in two. Firstly, a k×k convolution is separably applied to each input channel. This can be viewed as a group convolution where the number of groups corresponds to the number of channels C. In other words, there are C kernels k×k×1. Secondly, a 1×1 convolution is applied to propagate inter-channel information. Replacing standard convolutions with depthwise separable convolutions lowers the number of parameters and increases the inference speed at the cost of some drop in performance [xxref-bibbib10]. Strong regularization effect of depthwise separable convolutions can be relaxed by inverted residual blocks [xxref-bibbib31] which lead to compact residual models suitable for mobile applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2003"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Most semantic segmentation approaches aiming at real-time operation refrain from using encoders designed for competitive ImageNet performance. ICNet [xxref-bibbib40] proposes a custom encoder which processes an image pyramid with shared parameters and fuses multi-scale representations before entering the decoder which restores the resolution. ERFNet [xxref-bibbib28] redefines a residual block as a composition of a 3×1 followed by a 1×3 convolution, which yields a 33% reduction in parameters. Vallurupalli et al. [xxref-bibbib35] propose the DG2s approach as an efficient ERFNet variant with the following modifications in residual blocks: i) depthwise separable convolutions, and ii) channel shuffling operation before pointwise convolutions. ESPNet [xxref-bibbib24] factorizes convolutions in a similar manner and refrains from shared parameters across the image pyramid in order to produce a fast and compact architecture."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2004"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Our method is most related to semantic segmentation approaches which use lightweight encoders trained on ImageNet and benefit from such initialization. Similar to our work, Nekrasov et al. [xxref-bibbib25] rely on MobileNet V2 [xxref-bibbib31] and NASNet [xxref-bibbib42] encoders and feature a thick decoder with lateral connections. This is similar to our single scale model, however, our decoder has much less capacity, which allows us to report a half of their number of floating point operations without sacrificing recognition accuracy on road driving datasets. LinkNet [xxref-bibbib2] uses a small ResNet-18 backbone and a lightweight decoder to achieve satisfying performance-speed ratio. Our single scale model is similar to LinkNet, however we omit convolutional layers at full resolution in order to substantially reduce memory imprint and greatly increase the processing speed. Mazzini et al. [xxref-bibbib23] use a dilated encoder initialized from the DRN-D-22 model [xxref-bibbib38], and a decoder with one lateral connection. They also learn nonlinear upsampling to improve accuracy at object boundaries. Instead of using dilated convolutions, our decoder upsamples predictions by exclusively relying on lateral connections, which results in a 4-fold speed-up. Additionally, we succeed to leverage full resolution images during training which achieves an improvement of 5 percentage points on Cityscapes test."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2005"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S3 3 The proposed segmentation method"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Our method assumes the following requirements. The model should be based on an ImageNet pre-trained encoder in order to benefit from regularization induced by transfer learning. The decoder should restore the resolution of encoded features in order for the predictions to retain detail. The upsampling procedure must be as simple as possible in order to maintain real-time processing speed. Gradient flow should be promoted throughout the network to support training from random initialization in an unusual event that ImageNet pre-training turns out not to be useful."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "3000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S3SS1 3.1 Basic building blocks"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The proposed segmentation method is conceived around three basic building blocks which we briefly describe in the following paragraphs. These building blocks are going to be used in our two models which we propose in subsequent subsections."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-S3SS1SSS0Px1 Recognition encoder We advocate the usage of compact ImageNet pre-trained model as segmentation encoders. We propose to use ResNet-18 [xxref-bibbib8] and MobileNet V2 [xxref-bibbib31] for a number of reasons. These models are a good fit for fine tuning due to pre-trained parameters being publicly available. They are also suitable for training from scratch due to moderate depth and residual structure. Finally, these models are compatible with real-time operation due to small operation footprint. Computationally, ResNet-18 is around six times more complex than MobileNet V2. However, MobileNet V2 uses depthwise separable convolutions which are not directly supported in GPU firmware (the cuDNN library). Therefore, MobileNet V2 tends to be slower than ResNet-18 in most experimental setups. Note that the same issue disqualifies usage of the DenseNet architecture [xxref-bibbib12], since it requires efficient convolution over a non-contiguous tensor, which is still not supported in cuDNN."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4001"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-S3SS1SSS0Px2 Upsampling decoder The recognition encoder transforms the input image into semantically rich visual features. These features must have a coarse spatial resolution in order to save memory and processing time. The purpose of the decoder is to upsample these features to the input resolution. We advocate a simple decoder organized as a sequence of upsampling modules with lateral connections [xxref-bibbib27, xxref-bibbib29]. The proposed ladder-style upsampling modules have two inputs: the low resolution features (which should be upsampled), and the lateral features from an earlier layer of the encoder. The low resolution features are first upsampled with bilinear interpolation to the same resolution as the lateral features. Upsampled input features and lateral encoder features are then mixed with elementwise summation and finally blended with a 3×3 convolution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4002"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "We propose to route the lateral features from the output of the elementwise sum within the last residual block at the corresponding level of subsampling, as shown in Figure xxref-S3F2. Note that routing the lateral features from the output of the subsequent ReLU leads to a considerable drop in validation accuracy. Replacing the standard 3×3 convolution with either a 1×1 convolution, or a depthwise separable convolution also decreases the validation accuracy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4003"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-S3F2 Figure 2: Structural diagram of the last residual unit within a convolutional block operating on common spatial resolution. We do not use pre-activation [xxref-bibbib9] since we could not find a pre-trained parameterization for ResNet-18. The lateral connection is taken from the output of the elementwise sum after the last residual block. The output of the ReLU node is forwarded to the next residual block."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4004"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-S3SS1SSS0Px3 Module for increasing the receptive field As discussed before, there are two viable possibilities for increasing the receptive field while maintaining real-time speed: i) spatial pyramid pooling, and ii) pyramid fusion. The SPP block gathers the features produced by the encoder at several pooling levels and produces a representation with a varying level of detail. We demonstrate the use of SPP in our single scale model. Our SPP block is a simplified version of the pyramid pooling module from PSPNet [xxref-bibbib41]. The pyramid fusion produces genuine multi-scale representations which need to be carefully fused within the decoder in order to avoid overfitting to unsuitable level of detail. We propose a pyramid pooling approach which blends representations at different levels of abstraction and thus enlarges the receptive field without sacrificing spatial resolution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4005"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S3SS2 3.2 Single scale model"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The proposed single scale model transforms the input image into dense semantic predictions throughout a downsampling recognition encoder and upsampling decoder, as shown in Figure xxref-S3F3. Yellow trapezoids designate convolution groups, that is, parts of the encoder which operate on the same spatial resolution. All considered encoders consist of four such convolution groups. The first convolution group produces features at the H/4×W/4 resolution, while each following group increases the subsampling by the factor of 2. Thus the features at the far end of the encoder are H/32×W/32. These features are fed into the spatial pyramid pooling layer (designated by a green diamond) in order to increase the model receptive field. The resulting tensor is finally routed to the decoder whose upsampling modules are shown in blue."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "5000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Note that decoder and encoder are asymmetric: the encoder has many convolutions per convolution group while decoder has only one convolution per upsampling module. Furthermore, the dimensionality of encoder features increases along the downsampling path, while the dimensionality of the decoder features is constant. Therefore, lateral connections have to adjust dimensionality with 1×1 convolutions designated with red squares. Upsampling modules operate in three steps: i) the low resolution representation is bilinearly upsampled, ii) the obtained representations is summed with the lateral connection, iii) the summation is blended using a 3×3 convolution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "5001"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-S3F3 Figure 3: Structural diagram of the proposed single scale model. Yellow trapezoids designate convolution groups within the encoder which may be pre-trained on ImageNet. The green diamond designates the spatial pyramid pooling layer, the red squares designate bottleneck layers, and blue trapezoids designate lightweight upsampling modules. Logits are upsampled to original image resolution with bilinear interpolation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "5002"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S3SS3 3.3 Interleaved pyramid fusion model"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "While using a compact encoder is beneficial for fast inference, this also results in a decreased receptive field and a smaller capacity compared to general purpose convolutional models for visual recognition. To counteract these drawbacks, we propose to exploit image pyramids to enlarge the receptive field of the model and reduce the model capacity requirements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "6000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The proposed model is shown in Figure xxref-S3F4. Two encoder instances (yellow) are applied to the input image at different levels of the resolution pyramid. This results in increased receptive field of the activations which sense the lowest resolution of the image pyramid. Furthermore, shared parameters enable recognition of objects of different sizes with the common set of parameters, which may relax the demand for model capacity. In order to enforce lateral connections and improve the gradient flow throughout the encoder, we concatenate the feature tensors from neighbouring levels of different encoders (we can do that since they have equal spatial resolution). This concatenation is designated with green circles. After concatenation, interleaved encoder features are projected onto the decoder feature space by 1×1 convolutions designated with red squares. The decoder (blue) operates in the same manner as in the single-scale model, however now we have an additional upsampling module for each additional level of the image pyramid."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "6001"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-S3F4 Figure 4: Structural diagram of the proposed model with interleaved pyramidal fusion. Encoder parameters (yellow) are shared across all pyramid levels and may be pre-trained on Imagenet. Features of the same resolutions are concatenated (green circles), fed into a 1×1 bottleneck convolution (red squares) and blended within the decoder (blue)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "6002"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S4 4 Experiments"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "We conduct semantic segmentation experiments on two datasets: Cityscapes [xxref-bibbib5] and CamVid [xxref-bibbib1]. We report mIoU accuracy, computational complexity and the execution speed of the trained models. The speed measurements are performed on a desktop GPU (GTX 1080Ti) and on an embedded System on a chip module (Jetson TX2). We also present ablation and validation experiments which provide a more detailed insight into the impact of various design choices. Please note that additional experiments can be found in the supplement."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "7000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S4SS1 4.1 Training details"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "We train all our models with the Adam [xxref-bibbib14] optimizer with the learning rate set to 4⋅10−4. We decay the learning rate with cosine annealing [xxref-bibbib21] to the minimum value of 1⋅10−6 in the last epoch (we do not perform any warm restarts). The weight decay is set to 1⋅10−4. In experiments with ImageNet pre-training, we update pre-trained parameters with 4 times smaller learning rate and apply 4 times smaller weight decay. We train on jittered square crops with batch size 12. The jittering consists of random horizontal flipping, and scaling with random factors between 0.5 and 2. We use 768×768 crops for full Cityscapes resolution, and 448×448 crops for half Cityscapes resolution and CamVid. We train for 200 epochs on Cityscapes and 400 epochs on CamVid. We train for additional 200 epochs in experiments without ImageNet pre-training."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "8000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S4SS2 4.2 Measuring the computational complexity"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "We express the computational complexity of the trained models with two metrics: i) billions of floating point operations (GFLOP), and ii) number of processed frames per second (FPS). The GFLOP metric provides the number of fused multiply-add operations required to evaluate the model. Such platform-agnostic measure of the computational complexity is suitable for CPUs where all multiplications require roughly equal processing time. Unfortunately, the GFLOP metric poorly corresponds with the actual processing time on GPU platforms, since efficient implementations are available only for a small subset of all building blocks used to express current deep models. Consequently, both metrics are required for a complete description of algorithm suitability for real-time operation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "9000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The FPS metric directly corresponds to the processing time on a particular hardware platform. Such metric does not necessarily correlate across platforms, although rough estimations can be done, as we show below. We simulate real-time applications by setting batch size to 1. We measure the time elapsed between transferring the input data to the GPU, and receiving the semantic predictions into RAM as shown in the code snippet shown in Figure xxref-S4F5."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "9001"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-S4F5 ⬇ model.\\ttbeval() model.to(device) with torch.no_grad(): \\ttbinput = model.prepare_data(batch).to(device) logits = model.forward(\\ttbinput) torch.cuda.synchronize() t0 = 1000 * perf_counter() \\ttbfor _ \\ttbin \\ttbrange(n): \\ttbinput = model.prepare_data(batch).to(device) logits = model.forward(\\ttbinput) _, pred = logits.\\ttbmax(1) out = pred.data.byte().cpu() torch.cuda.synchronize() t1 = 1000 * perf_counter() fps = (1000 * n) / (t1 - t0) \\ttm Figure 5: Measurement of the processing time under PyTorch."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "9002"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "We conduct all measurements on a single GTX1080Ti with CUDA 10.0, CUDNN 7.3 and PyTorch 1.0rc1. We exclude the batch normalization layers [xxref-bibbib13] from measurements since in real-time applications they would be fused with preceding convolutional layers. We report mean FPS values over 1000 forward passes. Results are shown in Table xxref-S4T1. The column FPS norm provides a rough estimate on how would other methods perform on our hardware. The scaling factors are: 1.0 for GTX1080Ti, 0.61 for TitanX Maxwell, 1.03 for TitanX Pascal, and 1.12 for Titan XP. These scaling factors were calculated using publicly available benchmarks: goo.gl/N6ukTz, goo.gl/BaopYQ. The column GFLOPs@1MPx shows an estimated number of FLOPs for an input image of 1MPx, as a resolution-agnostic metric of computational complexity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "9003"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxtable-xxanchor-S4T1 Table 1: Results of semantic segmentation on Cityscapes. We evaluate our best result on the online test benchmark and compare it with relevant previous work, where possible. We also report the computational complexity (GFLOP, FPS) GPU on which the inference was performed, and the image resolution on which the training and inference were performed. The column GFLOPs@1Mpx shows the GFLOPs metric when the input resolution is 1MPx. The column FPS norm shows or estimates the FPS metric on GTX 1080Ti. The default SwiftNet configuration is the single scale model presented in xxref-S3SS2. Label pyr denotes the pyramid fusion model presented in xxref-S3SS3. Label ens denotes the ensemble of the single scale model and the pyramid model. The symbol † designates ImageNet pre-training."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "9004"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S4SS3 4.3 Cityscapes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The Cityscapes dataset is a collection of high resolution images taken from the driver’s perspective during daytime and fine weather. It consists of 2975 training, 500 validation, and 1525 test images with labels from 19 classes. It also provides 20000 coarsely labeled images which we do not use during our experiments. Table xxref-S4T1 evaluates the accuracy (class mIoU) and efficiency (GFLOP, FPS) of our methods and compares them to other real-time methods. Our single scale method based on the ResNet-18 encoder achieves 75.5% test mIoU, and delivers 39.9 FPS on full Cityscapes resolution (1024×2048 pixels). To the best of our knowledge, this result outperforms all other approaches aiming at real-time operation. The corresponding submission to the Cityscapes evaluation server is entitled SwiftNetRN-18. Table xxref-S4T1 also presents experiments in which our models are trained from scratch. The accuracy decreases for 5 mIoU percentage points (pp) with respect to the corresponding experiments with ImageNet pre-trained initialization. This shows that ImageNet pre-training represents an important ingredient for reaching highly accurate predictions. We notice that custom encoders like ERFNet [xxref-bibbib28] get less benefits from ImageNet pre-training: only 1.7% pp as shown in Table xxref-S4T1. Figure xxref-S4F7 presents examples of segmentations on Cityscapes val images. We show examples for both single scale and pyramid models. We did not achieve measurable improvements with the pyramid model over the single scale model on the Cityscapes dataset."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "10000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S4SS4 4.4 CamVid"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The CamVid dataset contains 701 densely annotated frames. We use the usual split into 367 train, 101 validation and 233 test images. We train on combined train and validation subsets and evaluate semantic segmentation into 11 classes on the test subset. Table xxref-S4T2 shows that we obtain an improvement of roughly 1.5 pp mIoU when using the pyramid model with pre-trained ResNet-18 and MobileNetV2 backbones. Figure xxref-S4F8 shows frames from the CamVid test subset where the pyramid model performed better."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "11000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Table xxref-S4T2 also shows that ImageNet pre-training contributes more on CamVid than on Cityscapes (7-9pp of mIoU performance). This is not surprising since CamVid has almost 20 times less training pixels."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "11001"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "A small size of the dataset poses a considerable challenge when training from scratch due to high overfitting risk. Table xxref-S4T2 shows that the pyramid model achieves better results than the single scale model. This supports our choice of sharing encoder parameters across pyramid levels."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "11002"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxtable-xxanchor-S4T2 Table 2: Semantic segmentation accuracy on CamVid test using ImageNet pre-training (mIoU†) and training from scratch (mIoU)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "11003"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S4SS5 4.5 Validation of the upsampling capacity"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The number of feature maps along the upsampling path is the most important design choice of the decoder. We validate this hyper-parameter and report the results in Table xxref-S4T3. The results show that the model accuracy saturates at 128 dimensions. Consequently, we pick this value as a sensible speed-accuracy trade-off in all other experiments."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "12000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxtable-xxanchor-S4T3 Table 3: Validation of the number of feature maps in the upsampling path. The models were trained on Cityscapes train subset at 512×1024 while the evaluation is performed on Cityscapes val. All models use ImageNet initialization."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "12001"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S4SS6 4.6 Ablation of lateral connections"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "To demonstrate the importance of lateral connections between the encoder and the decoder, we train a single scale model without lateral connections. For this experiment, we discard the 1×1 convolution layers located on the skip connections and abandon the elementwise summations in upsampling modules. Training such a model on full Cityscapes train images causes the validation accuracy to drop from 75.35% to 72.93%."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "13000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S4SS7 4.7 Execution profile"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "To obtain a better insight into the execution time of our models, we report separate processing times and the GFLOP metrics for the downsampling path (encoder and SPP), and the upsampling path (decoder). Table xxref-S4T4 shows the results for the single scale model and input resolution of 2048×1024. Note the striking discrepancy of time and GFLOPs for the two downsampling paths. ResNet-18 is almost twice as fast than MobileNet v2 despite requiring 6 times more multiplications. Note also that our decoder is twice as fast as the ResNet-18 encoder."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "14000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxtable-xxanchor-S4T4 Table 4: Inference speed along the downsampling (encoder and SPP) and the upsampling (decoder) paths for the single scale model. The columns dn time and up time display the execution times, while the columns dn FLOPs and up FLOPs show the number of floating point operations for 2048×1024 images."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "14001"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S4SS8 4.8 Size of the receptive field"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "We estimate the effective receptive field of our models by considering the central pixel in each image X of Cityscapes val. The estimate is based on gradients ∂yi∂X[xxref-bibbib22] where y are the logits for the central pixel while i is argmax(y). Table xxref-S4T5 expresses the effective receptive fields as standard deviations of pixels with top 5% gradient ∂yi∂X. The results show that both SPP and interleaved pyramidal fusion substantially increase the receptive field."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "15000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxtable-xxanchor-S4T5 Table 5: Effective receptive fields (ERF) expressed as standard deviation of pixels with top 5% image gradients with respect to the dominant class of the central pixel, measured on Cityscapes val."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "15001"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S4SS9 4.9 Processing speed on Jetson TX2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "We evaluate the proposed methods on NVIDIA Jetson TX2 module under CUDA 9.0, CUDNN 7.1, and PyTorch v1.0rc1. Due to limited number of CUDA cores, all bilinear interpolations had to be replaced with nearest neighbour interpolation. Results are reported in Figure xxref-S4F6. The MobileNet V2 backbone outperforms ResNet-18 for 20-30% on most resolutions due to lower number of FLOPs. However, ResNet-18 is faster on the lowest resolution. Note that our implementations do not use TensorRT optimizations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "16000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-S4F6 Figure 6: Processing speed in frames per second of the proposed architecture on NVIDIA Jetson TX2 module for two different backbones and various input resolutions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "16001"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-S4F7 Figure 7: Semantic segmentation results on Cityscapes val. The columns correspond to input image, ground truth annotation, the output of the pyramid model, and the output of the single scale model. The most significant improvements occur on pixels of the class truck."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "16002"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-S4F8 Figure 8: Semantic segmentation results on CamVid test. The columns correspond to input, ground truth, the output of the pyramid model, and the output of the single scale model. The most significant improvements occur on pixels of classes bus (top) and tree (bottom)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "16003"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-S5 5 Conclusion"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Real-time performance is a very important trait of semantic segmentation models aiming at applications in robotics and intelligent transportation systems. Most previous work in the field involves custom convolutional encoders trained from scratch, and decoders without lateral skip-connections. However, we argue that a better speed-accuracy trade-off is achieved with i) compact encoders designed for competitive ImageNet performance and ii) lightweight decoders with lateral skip-connections. Additionally, we propose a novel interleaved pyramidal fusion scheme which is able to further improve the results on large objects close to the camera. We provide a detailed analysis of prediction accuracy and processing time on Cityscapes and CamVid datasets for models based on ResNet-18 and MobileNetv2. Our Cityscapes test submission achieves 75.5% mIoU by processing 1024×2048 images at 39.9 Hz on a GTX1080Ti. To the best of our knowledge, this result outperforms all previous approaches aiming at real-time application. The source code is available at https://github.com/orsic/swiftnet."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "17000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-Sx1 Acknowledgment"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "This work has been supported by the European Regional Development Fund under the project System for increased driving safety in public urban rail traffic (SafeTRAM) under grant KK.01.2.1.01.0022, and by European Regional Development Fund (DATACROSS) under grant KK.01.1.1.01.0009, and Microblink Ltd. We would like to thank Josip Krapac for helpful discussions. The Titan Xp used to train some of the evaluated models was donated by NVIDIA Corporation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "18000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# xxanchor-bib References"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib1 [1] G. J. Brostow, J. Fauqueur, and R. Cipolla. Semantic object classes in video: A high-definition ground truth database. Pattern Recognition Letters, xx(x):xx–xx, 2008."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19000"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib2 [2] A. Chaurasia and E. Culurciello. Linknet: Exploiting encoder representations for efficient semantic segmentation. In 2017 IEEE Visual Communications and Image Processing, VCIP 2017, St. Petersburg, FL, USA, December 10-13, 2017, pages 1–4, 2017."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19001"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib3 [3] L. Chen, G. Papandreou, F. Schroff, and H. Adam. Rethinking atrous convolution for semantic image segmentation. CoRR, abs/1706.05587, 2017."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19002"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib4 [4] L. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille. Attention to scale: Scale-aware semantic image segmentation. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 3640–3649, 2016."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19003"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib5 [5] M. Cordts, M. Omran, S. Ramos, T. Scharwächter, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset. In CVPRW, 2015."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19004"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib6 [6] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical features for scene labeling. IEEE transactions on pattern analysis and machine intelligence, 35(8):1915–1929, 2013."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19005"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib7 [7] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. PAMI, 2015."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19006"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib8 [8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19007"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib9 [9] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, pages 630–645, 2016."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19008"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib10 [10] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. CoRR, abs/1704.04861, 2017."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19009"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib11 [11] G. Huang, S. Liu, L. van der Maaten, and K. Q. Weinberger. Condensenet: An efficient densenet using learned group convolutions. arXiv preprint arXiv:1711.09224, 2017."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19010"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib12 [12] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 2261–2269, 2017."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19011"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib13 [13] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, pages 448–456, 2015."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19012"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib14 [14] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19013"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib15 [15] I. Kreso, D. Causevic, J. Krapac, and S. Segvic. Convolutional scale invariance for semantic segmentation. In GCPR, pages 64–75, 2016."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19014"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib16 [16] I. Kreso, J. Krapac, and S. Segvic. Ladder-style densenets for semantic segmentation of large natural images. In ICCVW CVRSUAD, pages 238–245, 2017."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19015"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib17 [17] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In CVPR, pages 2169–2178, 2006."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19016"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib18 [18] G. Lin, A. Milan, C. Shen, and I. D. Reid. Refinenet: Multi-path refinement networks for high-resolution semantic segmentation. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 5168–5177, 2017."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19017"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib19 [19] T.-Y. Lin, P. Dollár, R. B. Girshick, K. He, B. Hariharan, and S. J. Belongie. Feature pyramid networks for object detection. In CVPR, volume 1, page 4, 2017."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19018"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib20 [20] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431–3440, 2015."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19019"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib21 [21] I. Loshchilov and F. Hutter. SGDR: stochastic gradient descent with restarts. CoRR, abs/1608.03983, 2016."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19020"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib22 [22] W. Luo, Y. Li, R. Urtasun, and R. S. Zemel. Understanding the effective receptive field in deep convolutional neural networks. In NIPS, pages 4898–4906, 2016."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19021"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib23 [23] D. Mazzini. Guided upsampling network for real-time semantic segmentation. In British Machine Vision Conference 2018, BMVC 2018, Northumbria University, Newcastle, UK, September 3-6, 2018, page 117, 2018."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19022"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib24 [24] S. Mehta, M. Rastegari, A. Caspi, L. G. Shapiro, and H. Hajishirzi. Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation. In Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part X, pages 561–580, 2018."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19023"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib25 [25] V. Nekrasov, C. Shen, and I. D. Reid. Light-weight refinenet for real-time semantic segmentation. In British Machine Vision Conference 2018, BMVC 2018, Northumbria University, Newcastle, UK, September 3-6, 2018, page 125, 2018."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19024"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib26 [26] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and transferring mid-level image representations using convolutional neural networks. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pages 1717–1724, 2014."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19025"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib27 [27] A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and T. Raiko. Semi-supervised learning with ladder networks. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 3546–3554, 2015."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19026"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib28 [28] E. Romera, J. M. Alvarez, L. M. Bergasa, and R. Arroyo. Erfnet: Efficient residual factorized convnet for real-time semantic segmentation. IEEE Transactions on Intelligent Transportation Systems, 19(1):263–272, 2018."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19027"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib29 [29] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015 - 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part III, pages 234–241, 2015."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19028"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib30 [30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19029"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib31 [31] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and L. Chen. Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation. CoRR, abs/1801.04381, 2018."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19030"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib32 [32] M. Siam, M. Gamal, M. Abdel-Razek, S. Yogamani, M. Jagersand, H. Zhang, N. Vallurupalli, S. Annamaneni, G. Varma, C. Jawahar, et al. A comparative study of real-time semantic segmentation for autonomous driving. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 587–597, 2018."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19031"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib33 [33] L. Sifre and S. Mallat. Rigid-motion scattering for texture classification. CoRR, abs/1403.1687, 2014."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19032"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib34 [34] B. Singh and L. S. Davis. An analysis of scale invariance in object detection–snip. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3578–3587, 2018."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19033"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib35 [35] N. Vallurupalli, S. Annamaneni, G. Varma, C. Jawahar, M. Mathew, and S. Nagori. Efficient semantic segmentation using gradual grouping. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2018."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19034"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib36 [36] M. Wang, B. Liu, and H. Foroosh. Factorized convolutional neural networks. In ICCV Workshops, pages 545–553, 2017."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19035"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib37 [37] F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. In ICLR, 2016."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19036"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib38 [38] F. Yu, V. Koltun, and T. A. Funkhouser. Dilated residual networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 636–644, 2017."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19037"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib39 [39] X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19038"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib40 [40] H. Zhao, X. Qi, X. Shen, J. Shi, and J. Jia. Icnet for real-time semantic segmentation on high-resolution images. arXiv preprint arXiv:1704.08545, 2017."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19039"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib41 [41] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene parsing network. In ICCV, 2017."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19040"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-bibbib42 [42] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le. Learning transferable architectures for scalable image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "19041"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paper.text.fragments.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxanchor-S1 1 Introduction\n",
      "xxanchor-S2 2 Related Work\n",
      "xxanchor-S3 3 The proposed segmentation method\n",
      "xxanchor-S3SS1 3.1 Basic building blocks\n",
      "xxanchor-S3SS2 3.2 Single scale model\n",
      "xxanchor-S3SS3 3.3 Interleaved pyramid fusion model\n",
      "xxanchor-S4 4 Experiments\n",
      "xxanchor-S4SS1 4.1 Training details\n",
      "xxanchor-S4SS2 4.2 Measuring the computational complexity\n",
      "xxanchor-S4SS3 4.3 Cityscapes\n",
      "xxanchor-S4SS4 4.4 CamVid\n",
      "xxanchor-S4SS5 4.5 Validation of the upsampling capacity\n",
      "xxanchor-S4SS6 4.6 Ablation of lateral connections\n",
      "xxanchor-S4SS7 4.7 Execution profile\n",
      "xxanchor-S4SS8 4.8 Size of the receptive field\n",
      "xxanchor-S4SS9 4.9 Processing speed on Jetson TX2\n",
      "xxanchor-S5 5 Conclusion\n",
      "xxanchor-Sx1 Acknowledgment\n",
      "xxanchor-bib References\n"
     ]
    }
   ],
   "source": [
    "for header in paper.text.fragments.get_toc():\n",
    "    print(header)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
