{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/github/mkardas/paper-extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='952' class='' max='952', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [952/952 00:03<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='948' class='' max='948', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [948/948 00:23<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sota_extractor2.data.paper_collection import PaperCollection\n",
    "from pathlib import Path\n",
    "\n",
    "#DATA_PATH = Path(\"/home/ubuntu/pwc/arxiv-s3/arxiv\")\n",
    "DATA_PATH = Path(\"/home/ubuntu/pwc/arxiv-pwc/arxiv\")\n",
    "\n",
    "pc = PaperCollection(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKLE_PATH = Path(\"/home/ubuntu/pwc/pc-pickle-fast.pkl\")\n",
    "pc.to_pickle(PICKLE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.78 s, sys: 149 ms, total: 2.93 s\n",
      "Wall time: 2.9 s\n"
     ]
    }
   ],
   "source": [
    "%time pc2 = PaperCollection.from_pickle(PICKLE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>body{margin:0;padding:0;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue,sans-serif;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}code{font-family:source-code-pro,Menlo,Monaco,Consolas,Courier New,monospace}.tableWrapper{-overflow:auto}.tableWrapper .model-params{background-color:#209cee;color:rgba(0,0,0,.7)}.tableWrapper .table-meta{background-color:#fff3c5;color:rgba(0,0,0,.7)}.tableWrapper .model-best{background-color:#ff3860;color:rgba(0,0,0,.7)}.tableWrapper .model-competing{background-color:#ffdd57;color:rgba(0,0,0,.7)}.tableWrapper .model-paper{background-color:#ff3860;color:#fff}.tableWrapper .dataset-sub{background-color:#23d160;color:#fff}.tableWrapper .dataset-metric{background-color:#209cee;color:#fff}.tableWrapper .dataset{background-color:#02bd43;color:#fff}.tableWrapper .trash{background-color:#363636;color:#f5f5f5}.tableWrapper .wtf{background-color:#f0f;color:#f5f5f5}.tableWrapper .dataset-task{background-color:#77ecdd;color:rgba(0,0,0,.7)}.tableWrapper .dataset-paper{background-color:#e4ffee;color:rgba(0,0,0,.7)}.tableWrapper td.focused-cell{outline:2px solid #9ecaed;border-radius:7px;box-shadow:0 0 10px #9ecaed}div.form-group>input.form-control.input-sm{border-radius:2px;font-size:.75rem;background-color:#fff;color:#363636;box-shadow:inset 0 1px 2px rgba(10,10,10,.1);max-width:100%;width:100%;height:2.25em;padding:calc(.375em - 1px) calc(.625em - 1px);position:relative;border:1px solid #b5b5b5}div.form-group>input.form-control.input-sm:focus{border-color:#3273dc;box-shadow:0 0 0 .125em rgba(50,115,220,.25)}\n",
       "</style>\n",
       "\n",
       "<div class=\"tableWrapper\">\n",
       "<table>\n",
       "<tr>\n",
       "<td class=\"table-meta\">Model</td>\n",
       "<td class=\"model-params\">d</td>\n",
       "<td class=\"model-params\">|Î¸|M</td>\n",
       "<td class=\"dataset-sub\">Train</td>\n",
       "<td class=\"dataset-sub\">Test</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\"model-competing\">Classifier with handcrafted features [12]</td>\n",
       "<td class=\"\">-</td>\n",
       "<td class=\"\">-</td>\n",
       "<td class=\"\">99.7</td>\n",
       "<td class=\"\">78.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\"model-competing\">LSTM encoders [12]</td>\n",
       "<td class=\"\">300</td>\n",
       "<td class=\"\">3.0M</td>\n",
       "<td class=\"\">83.9</td>\n",
       "<td class=\"\">80.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\"model-competing\">Dependency Tree CNN encoders [13]</td>\n",
       "<td class=\"\">300</td>\n",
       "<td class=\"\">3.5M</td>\n",
       "<td class=\"\">83.3</td>\n",
       "<td class=\"\">82.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\"model-competing\">SPINN-PI encoders [14]</td>\n",
       "<td class=\"\">300</td>\n",
       "<td class=\"\">3.7M</td>\n",
       "<td class=\"\">89.2</td>\n",
       "<td class=\"\">83.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\"model-paper\">NSE</td>\n",
       "<td class=\"\">300</td>\n",
       "<td class=\"\">3.4M</td>\n",
       "<td class=\"\">86.2</td>\n",
       "<td class=\"\">84.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\"model-paper\">MMA-NSE</td>\n",
       "<td class=\"\">300</td>\n",
       "<td class=\"\">6.3M</td>\n",
       "<td class=\"\">87.1</td>\n",
       "<td class=\"\">84.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\"model-competing\">LSTM attention [15]</td>\n",
       "<td class=\"\">100</td>\n",
       "<td class=\"\">242K</td>\n",
       "<td class=\"\">85.4</td>\n",
       "<td class=\"\">82.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\"model-competing\">LSTM word-by-word attention [15]</td>\n",
       "<td class=\"\">100</td>\n",
       "<td class=\"\">252K</td>\n",
       "<td class=\"\">85.3</td>\n",
       "<td class=\"\">83.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\"model-best\">MMA-NSE attention</td>\n",
       "<td class=\"\">300</td>\n",
       "<td class=\"\">6.5M</td>\n",
       "<td class=\"\">86.9</td>\n",
       "<td class=\"\">85.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\"model-competing\">mLSTM word-by-word attention [16]</td>\n",
       "<td class=\"\">300</td>\n",
       "<td class=\"\">1.9M</td>\n",
       "<td class=\"\">92.0</td>\n",
       "<td class=\"\">86.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\"model-competing\">LSTMN with deep attention fusion [17]</td>\n",
       "<td class=\"\">450</td>\n",
       "<td class=\"\">3.4M</td>\n",
       "<td class=\"\">89.5</td>\n",
       "<td class=\"\">86.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\"model-competing\">Decomposable attention model [18]</td>\n",
       "<td class=\"\">200</td>\n",
       "<td class=\"\">582K</td>\n",
       "<td class=\"\">90.5</td>\n",
       "<td class=\"\">86.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td class=\"model-competing\">Full tree matching NTI-SLSTM-LSTM global attention [19]</td>\n",
       "<td class=\"\">300</td>\n",
       "<td class=\"\">3.2M</td>\n",
       "<td class=\"\">88.5</td>\n",
       "<td class=\"\">87.3</td>\n",
       "</tr>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paper = pc.papers['1607.04315']\n",
    "table = paper.tables[0]\n",
    "table.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cell(value='Classifier with handcrafted features [12]', gold_tags='model-competing', refs=['xxref-Xbowman:15'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.df.iloc[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sota'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.gold_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstract We present a memory augmented neural network for natural language understanding: Neural Semantic Encoders. NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves over time and maintains the understanding of input sequences through read , compose and write operations. NSE can also access 1 xxanchor-x1-2f1 multiple and shared memories. In this paper, we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks: natural language inference, question answering, sentence classification, document sentiment analysis and machine translation where NSE achieved state-of-the-art performance when evaluated on publically available benchmarks. For example, our shared-memory model showed an encouraging result on neural machine translation, improving an attention-based baseline by approximately 1.0 BLEU.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.text.abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1 xxanchor-x1-10001 Introduction"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2 xxanchor-x1-20002 Related Work"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "3 xxanchor-x1-30003 Proposed Approach"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "3.1 xxanchor-x1-40003.1 Read, Compose and Write"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "3.2 xxanchor-x1-50003.2 Shared and Multiple Memory Accesses"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4 xxanchor-x1-60004 Experiments"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4.1 xxanchor-x1-70004.1 Natural Language Inference"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4.2 xxanchor-x1-80004.2 Answer Sentence Selection"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4.3 xxanchor-x1-90004.3 Sentence Classification"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4.4 xxanchor-x1-100004.4 Document Sentiment Analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4.5 xxanchor-x1-110004.5 Machine Translation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "5.1 xxanchor-x1-130005.1 Memory Access and Compositionality"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "6 xxanchor-x1-140006 Conclusion"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-x1-150006 Acknowledgments"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "xxanchor-x1-160006 References"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "A xxanchor-x1-17000A Step-by-step visualization of memory states in NSE"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paper.text.print_toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# 1 xxanchor-x1-10001 Introduction,\n",
       "Recently several studies have explored ways of extending the neural networks with an external memory [ xxref-Xgraves2014neural – xxref-Xgrefenstette2015learning ]. Unlike LSTM, the short term memories and the training parameters of such a neural network are no longer coupled and can be adapted. In this paper we propose a novel class of memory augmented neural networks called Neural Semantic Encoders (NSE) for natural language understanding. NSE offers several desirable properties. NSE has a variable sized encoding memory which allows the model to access entire input sequence during the reading process; therefore efficiently delivering long-term dependencies over time. The encoding memory evolves over time and maintains the memory of the input sequence through read , compose and write operations. NSE sequentially processes the input and supports word compositionality inheriting both temporal and hierarchical nature of human language. NSE can read from and write to a set of relevant encoding memories simultaneously or multiple NSEs can access a shared encoding memory effectively supporting knowledge and representation sharing. NSE is flexible, robust and suitable for practical NLU tasks and can be trained easily by any gradient descent optimizer.<Fragment(meta.id=1607.04315_1001, order=1001)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.text.fragments[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
